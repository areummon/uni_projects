{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/areummon/uni_projects/blob/main/ProyectoRedesNeuronales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proyecto Final Redes Neuronales: Generador de Haikus\n",
        "\n",
        "## Ramón Arcos Morales: 319541478\n",
        "\n",
        "## Victor Manuel Casarrubias Casarrubias: 421003581"
      ],
      "metadata": {
        "id": "2tXXmu_X82Y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota: subir el archivo 'segundo.pth' que se adjunta con este notebook al almacenamiento de la sesión."
      ],
      "metadata": {
        "id": "kLgHrruZHYp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero importamos y descargamos el dataset de Haikus de Kaggle."
      ],
      "metadata": {
        "id": "svLHyiDwQX82"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5mRYzdkz6A1",
        "outputId": "8bc4b5c5-22a5-4864-ed5e-f1b07cbc0aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/bfbarry/haiku-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 331k/331k [00:00<00:00, 57.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/bfbarry/haiku-dataset/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"bfbarry/haiku-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los imports necesarios para manejar el modelo y formatear el dataset."
      ],
      "metadata": {
        "id": "2YZEN3XrQm2B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqe_I_7p1V9M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "Formateamos el dataset de modo que se pueda utilizar en el modelo de Transformer posteriormente definido.\n"
      ],
      "metadata": {
        "id": "7_xMMosJQ1mD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GekbHtwm0H0t"
      },
      "outputs": [],
      "source": [
        "with open(path+'/lines.txt', 'r', encoding='utf-8') as file:\n",
        "    haikus = file.read()\n",
        "# Primero se tiene que tokenizar el texto, siendo la elección (aunque por supuesto no la mejor)\n",
        "# la de codificar cada palabra como un entero.\n",
        "words = haikus.split()\n",
        "word_counts = Counter(words)\n",
        "vocab = list(word_counts.keys())\n",
        "vocab_size = len(vocab)\n",
        "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "# Deicidimos usar 64 como sequence length y context window pues vimos\n",
        "# que funcionaba bien y el entrenamiento era más eficiento con el tamaño de los batches de 32.\n",
        "SEQUENCE_LENGTH = 64\n",
        "samples = [words[i:i+SEQUENCE_LENGTH+1] for i in range(len(words)-SEQUENCE_LENGTH)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea la clase del dataset para poder usarlo en el Transformer de mejor manera."
      ],
      "metadata": {
        "id": "MVENBxNiSDP0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk1a2lTK31eJ"
      },
      "outputs": [],
      "source": [
        "class HaikuDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Clase Dataset para cargar y preprocesar haikus. Convierte secuencias de palabras\n",
        "    en tensores de enteros, listos para el modelo de Transformer.\n",
        "\n",
        "    Args:\n",
        "        samples (list): Lista de listas de palabras, donde cada sublista es una secuencia\n",
        "                        de palabras (haikus) de longitud `SEQUENCE_LENGTH + 1`.\n",
        "        word_to_int (dict): Diccionario que mapea cada palabra a un entero único.\n",
        "    \"\"\"\n",
        "    def __init__(self, samples, word_to_int):\n",
        "        self.samples = samples\n",
        "        self.word_to_int = word_to_int\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Retorna el número total de muestras en el dataset.\n",
        "        \"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retorna un par (input_seq, target_seq) para el índice dado.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Índice de la muestra.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Un par de tensores (input_seq, target_seq).\n",
        "                   - `input_seq`: Secuencia de palabras de entrada convertida a enteros.\n",
        "                   - `target_seq`: Secuencia de palabras objetivo (desplazada una palabra)\n",
        "                                   convertida a enteros.\n",
        "        \"\"\"\n",
        "        sample = self.samples[idx]\n",
        "        input_seq = torch.LongTensor([self.word_to_int[word] for word in sample[:-1]])\n",
        "        target_seq = torch.LongTensor([self.word_to_int[word] for word in sample[1:]])\n",
        "        return input_seq, target_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos el tamaño de los lotes o mini-lotes para el entrenamiento."
      ],
      "metadata": {
        "id": "aYArdSa5ShrJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghjLQBb74CqT"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "dataset = HaikuDataset(samples, word_to_int)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "En esta sección se define el modelo de Transforme Decoder-Only pues lo que se busca es solo generar texto a partir del entrenamiento con Haikus."
      ],
      "metadata": {
        "id": "0idrp2nPS3sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capa de atención enmascarada"
      ],
      "metadata": {
        "id": "_GNEvRlUThOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD-jl08W4qEl"
      },
      "outputs": [],
      "source": [
        "def mask_attention(sequence):\n",
        "    \"\"\"\n",
        "    Crea una máscara triangular superior para los mecanismos de atención del Transformer Decoder-Only.\n",
        "    Esto asegura que cada posición solo pueda atender a posiciones anteriores (incluyéndose a sí misma).\n",
        "\n",
        "    Args:\n",
        "        sequence (int): La longitud de la secuencia para la cual se generará la máscara.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Un tensor de PyTorch con la máscara de atención. Los valores en la parte\n",
        "                      superior derecha (posiciones futuras) se establecen en `-inf`, los permitidos en `0.0`.\n",
        "    \"\"\"\n",
        "    mask = (torch.triu(torch.ones(sequence, sequence)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding y codificación posicional"
      ],
      "metadata": {
        "id": "nmzpqI22TuCV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhoIvecj4rUi"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementa la codificación posicional para inyectar información sobre la posición relativa\n",
        "    de las palabras en la secuencia de entrada del Transformer.\n",
        "\n",
        "    Args:\n",
        "        max_len (int): Longitud máxima de la secuencia que el modelo puede manejar.\n",
        "        d_model (int): Dimensión del espacio de embeddings (y del modelo Transformer).\n",
        "        dropout (float, optional): Tasa de dropout aplicada a las codificaciones posicionales. Defaults to 0.1.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_len, d_model, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Aplica la codificación posicional al tensor de embeddings de entrada.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): El tensor de embeddings de entrada.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: El tensor de embeddings de entrada con la información posicional\n",
        "                          añadida y dropout aplicado.\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo de generación de texto Decoder Only"
      ],
      "metadata": {
        "id": "0BfE5wzxT4Wn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxfrzh_N4072"
      },
      "outputs": [],
      "source": [
        "class DecoderOnly(nn.Module):\n",
        "    \"\"\"\n",
        "    Define la arquitectura del modelo Transformer Decoder-Only para generación de texto.\n",
        "    Utiliza embeddings de palabras, codificación posicional y capas de decodificador\n",
        "    Transformer apiladas para predecir la siguiente palabra en una secuencia.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): El tamaño del vocabulario (número total de palabras únicas).\n",
        "        embed_dim (int): La dimensión de los embeddings de palabras y del modelo.\n",
        "        num_layers (int): El número de capas de decodificador Transformer apiladas.\n",
        "        num_heads (int): El número de cabezas en el mecanismo de autoatención multi-cabeza.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads):\n",
        "        super(DecoderOnly, self).__init__()\n",
        "        #Embedding y codificación posicional\n",
        "        self.pos_encoder = PositionalEncoding(max_len=SEQUENCE_LENGTH, d_model=embed_dim)\n",
        "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        #Capa linear para multi cabezas\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            decoder_layer=self.decoder_layer,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Pasa el tensor de entrada a través del modelo Decoder-Only.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): El tensor de entrada que contiene los índices de las palabras.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Un tensor de logits con las probabilidades de la siguiente palabra\n",
        "                          para cada posición en la secuencia.\n",
        "        \"\"\"\n",
        "        emb = self.emb(x)\n",
        "\n",
        "        input_mask = mask_attention(x.size(1)).to(x.device)\n",
        "\n",
        "        x = self.pos_encoder(emb)\n",
        "        x = self.decoder(x, memory=x, tgt_mask=input_mask, memory_mask=input_mask)\n",
        "        x = self.dropout(x)\n",
        "        out = self.linear(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "s2TSSC1EUMaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos el dispositivo de hardware que se usará para el entrenamiento y luego definimos los hiperparámetros y nuestro modelo de Transformer."
      ],
      "metadata": {
        "id": "zj24AjqVhB7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggmy6_dT4_bW",
        "outputId": "00a19183-bb5f-4fb3-e389-00bf987d4b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using CPU.\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA (NVIDIA GPU) is available!\")\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    print(\"MPS (Apple Silicon GPU) is available!\")\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = DecoderOnly(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=100,\n",
        "    num_layers=2,\n",
        "    num_heads=2,\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#Variable para ver el número de parámetros que serán entrandos\n",
        "#Se usaron para el reporte\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El entrenamiento, los pesos obtenidos de nuestro entrenamiento se cargan más adelante. No ejecutar esta celda"
      ],
      "metadata": {
        "id": "EVb6iZdBhafv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "Xl2O2gmb5bJK",
        "outputId": "798cae95-d33a-4176-9e9c-cfb585350b0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [01:53<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-247305879.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch} loss: {epoch_loss:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-247305879.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, dataloader, criterion)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1772596817.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    629\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 )\n\u001b[1;32m   1130\u001b[0m             )\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;31m# feed forward block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "#Entrenamiento\n",
        "def train(model, epochs, dataloader, criterion):\n",
        "    \"\"\"\n",
        "    Implementa el bucle de entrenamiento para el modelo DecoderOnly. Itera sobre los datos\n",
        "    en el `dataloader` para un número específico de épocas, calcula la pérdida,\n",
        "    realiza la retropropagación y actualiza los pesos del modelo.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): La instancia del modelo `DecoderOnly` a entrenar.\n",
        "        epochs (int): El número de épocas para entrenar el modelo.\n",
        "        dataloader (DataLoader): Un `DataLoader` que proporciona lotes de secuencias\n",
        "                                 de entrada y objetivo.\n",
        "        criterion (nn.Module): La función de pérdida (por ejemplo, `CrossEntropyLoss`).\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        running_loss = 0\n",
        "        for input_seq, target_seq in dataloader:\n",
        "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
        "            outputs = model(input_seq)\n",
        "            target_seq = target_seq.contiguous().view(-1)\n",
        "            outputs = outputs.view(-1, vocab_size)\n",
        "\n",
        "            loss = criterion(outputs, target_seq.view(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.detach().cpu().numpy()\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch} loss: {epoch_loss:.3f}\")\n",
        "train(model, epochs, dataloader, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación"
      ],
      "metadata": {
        "id": "Z6tM-7Gz1w_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los pesos del modelo previamente entrenado."
      ],
      "metadata": {
        "id": "SIEKWaCmjDJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/segundo.pth\",map_location=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtI8PU9oi_7C",
        "outputId": "c2990f60-3594-4ee9-8cd5-5b18d112a271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependencias necesarias para poder realizar la evaluación"
      ],
      "metadata": {
        "id": "7WiH4qmHYv-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install evaluate\n",
        "!pip3 install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQQUEBHr15dV",
        "outputId": "291a36fd-6b4f-4c5d-9a05-7224ae99de5c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=529a2f79512497b65099c18229d941aae0e43f6be2bf28bc336dd11d4d8542b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones de generación de haikus, hacemos uso de Top-p sampling para que el modelo siempre elija un token siguiente de los que tienen mayor probabilidad, esto para evitar que el modelo se cicle meidante una misma palabra, además nos aseguramos que siempre genere un haikú de al menos tres versos."
      ],
      "metadata": {
        "id": "jzIO7aF9YzlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_next_evaluation(predictions, p=0.9):\n",
        "    \"\"\"\n",
        "    Realiza un muestreo de Top-p (nucleus sampling) para seleccionar la siguiente palabra\n",
        "    durante la generación de texto. Asegura que el modelo elija entre un conjunto de\n",
        "    palabras con la mayor probabilidad acumulada `p`, promoviendo la diversidad en la\n",
        "    generación sin desviarse demasiado de las predicciones de alta probabilidad.\n",
        "\n",
        "    Args:\n",
        "        predictions (torch.Tensor): El tensor de logits (`[batch_size, sequence_length, vocab_size]`)\n",
        "                                    de la salida del modelo.\n",
        "        p (float, optional): El umbral de probabilidad para el muestreo de Top-p. Defaults to 0.9.\n",
        "\n",
        "    Returns:\n",
        "        int: El índice de la palabra seleccionada.\n",
        "    \"\"\"\n",
        "    probabilities = F.softmax(predictions[:, -1, :], dim=-1).cpu()\n",
        "    sorted_probs, sorted_indices = torch.sort(probabilities, descending=True, dim=-1)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    sorted_indices_to_remove = cumulative_probs > p\n",
        "    sorted_indices_to_remove[..., 0] = False\n",
        "    sorted_probs[sorted_indices_to_remove] = 0.0\n",
        "    sorted_probs = sorted_probs / sorted_probs.sum()\n",
        "    sampled_index = torch.multinomial(sorted_probs, num_samples=1)\n",
        "    next_token = sorted_indices[0][sampled_index]\n",
        "    return int(next_token.cpu())\n",
        "\n",
        "def haiku_generatior_evaluation(sentence, generate_length):\n",
        "    \"\"\"\n",
        "    Genera un haiku completo utilizando el modelo entrenado y la estrategia de muestreo\n",
        "    `sample_next_evaluation`. Diseñada para evaluación, intenta generar un haiku de tres\n",
        "    versos y asegura que termine con el token de fin de secuencia (`$`).\n",
        "\n",
        "    Args:\n",
        "        sentence (str): La palabra o frase inicial (prompt) para comenzar la generación.\n",
        "        generate_length (int): La longitud máxima de palabras a generar.\n",
        "\n",
        "    Returns:\n",
        "        str: El haiku generado como una cadena de texto, con los separadores de verso (`/`)\n",
        "             reemplazados por saltos de línea y el token `$` eliminado.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    sample = sentence\n",
        "    for i in range(generate_length):\n",
        "        int_vector = return_int_vector(sample)\n",
        "        if len(int_vector) >= SEQUENCE_LENGTH - 1:\n",
        "            break\n",
        "        input_tensor = int_vector.to(device)\n",
        "        with torch.no_grad():\n",
        "            predictions = model(input_tensor)\n",
        "        next_token = sample_next_evaluation(predictions)\n",
        "        sample += ' ' + int_to_word[next_token]\n",
        "        if (int_to_word[next_token] == '$'):\n",
        "            break\n",
        "    if (sample.count('/')<2):\n",
        "      return haiku_generatior_evaluation(sentence, generate_length)\n",
        "    if not sample.endswith('$'):\n",
        "        sample += ' $'\n",
        "    return sample.replace('\\n','/')\n",
        "\n",
        "def return_int_vector(text):\n",
        "  \"\"\"\n",
        "  Toma una cadena de texto y la convierte en un tensor de enteros, representando\n",
        "  los índices de las palabras en el vocabulario. Asegura que la secuencia de\n",
        "  entrada no exceda `SEQUENCE_LENGTH` palabras, tomando las últimas si es necesario.\n",
        "\n",
        "  Args:\n",
        "      text (str): La cadena de texto a convertir.\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor: Un tensor de PyTorch (`torch.LongTensor`) con los índices de las palabras.\n",
        "  \"\"\"\n",
        "  words = text.split()\n",
        "  input_seq = torch.LongTensor([word_to_int[word] for word in words[-SEQUENCE_LENGTH:]]).unsqueeze(0)\n",
        "  return input_seq"
      ],
      "metadata": {
        "id": "dpcdML4KHlzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La evaluación del modelo usando Rouge, hacemos una lista de 500 haikus de referencia que se usarán para probar el modelo y los comparamos con los haikus generados por el modelo a partir de las primeras palabras de la lista de referencia de cada haiku."
      ],
      "metadata": {
        "id": "TJld05LWZbD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "haiku_parts = haikus.split('\\n')\n",
        "haikus_de_referencia = [haiku_part.strip() for i,haiku_part in enumerate(haiku_parts) if i <= 500]\n",
        "\n",
        "haikus_generados = []\n",
        "num_samples = len(haikus_de_referencia)\n",
        "\n",
        "starting_prompts = [haiku.split()[0] for haiku in haikus_de_referencia]\n",
        "\n",
        "for i in range(num_samples):\n",
        "    prompt = starting_prompts[i % len(starting_prompts)]\n",
        "    generated = haiku_generatior_evaluation(prompt, generate_length=20)\n",
        "    haikus_generados.append(generated.strip())\n",
        "# Se calculan los valores de la evluación Rouge\n",
        "results = rouge.compute(\n",
        "    predictions=haikus_generados,\n",
        "    references=haikus_de_referencia,\n",
        "    use_stemmer=True\n",
        ")\n",
        "\n",
        "# Se imprimen los resultados de la evaluación\n",
        "print(\"\\nEvaluación Rouge:\")\n",
        "print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {results['rougeL']:.4f}\")\n",
        "print(f\"ROUGE-Lsum: {results['rougeLsum']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "528cc55d5b1741919daa095ebaee4696",
            "c460975c1dc2432a8843fec90e06a5e1",
            "dbe6bf9e6db94744b5335e7612e399fb",
            "54352b05d78d4276b685ea5aaba82453",
            "5469188f5fdb46f4bfefd7f45064504c",
            "527088779b87474784f50370cd1a6c91",
            "bed5dd1809854e9cad95e50ba42d492d",
            "021ae598b77f45049b9a40dfa6565d0b",
            "5509ad41830047daa019611d39b9401a",
            "e769f83f1bc84c559ca4399fc5887d69",
            "05882c0b2bbb4cfd969006cafac52755"
          ]
        },
        "id": "1-_ivoNy19O9",
        "outputId": "c3455e91-23a1-43df-c9eb-07d684451ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:86: UserWarning: \n",
            "Access to the secret `HF_TOKEN` has not been granted on this notebook.\n",
            "You will not be requested again.\n",
            "Please restart the session if you want to be prompted again.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "528cc55d5b1741919daa095ebaee4696"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluación Rouge:\n",
            "ROUGE-1: 0.2181\n",
            "ROUGE-2: 0.0867\n",
            "ROUGE-L: 0.2141\n",
            "ROUGE-Lsum: 0.2136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecución"
      ],
      "metadata": {
        "id": "ctfAj6VtUSgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero cargamos (de nuevo en caso de que no se haya hecho el entrenamiento) los pesos del entrenamiento anteriormente hecho"
      ],
      "metadata": {
        "id": "OxUtlp_VUXFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/segundo.pth\",map_location=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLv0i3TByvZr",
        "outputId": "c88adf3f-0751-43db-fc86-e275cf389e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se generan los haikus de tal manera que elija aleatoriamente entre los dos tokens más probables para la siguiente palabra y además, que contenga exactamente dos \"/\" el cual es el separador de versos, es decir, que un haiku siempre se conforme de tres versos.\n",
        "\n",
        "Es distinto a los métodos generativos de la evaluación ya que aquí se procupa generar haikus que sean diferentes o \"creativos\" a los originales, siendo la biblioteca random usada para este fin. Así, en cada paso, se elige aleatoriamente entre los tokens más probables"
      ],
      "metadata": {
        "id": "iG3YGFaOUceN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg5hHRx17aFi"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def return_int_vector(text):\n",
        "    \"\"\"\n",
        "    Toma una cadena de texto y la convierte en un tensor de enteros, representando\n",
        "    los índices de las palabras en el vocabulario. Asegura que la secuencia de\n",
        "    entrada no exceda `SEQUENCE_LENGTH` palabras, tomando las últimas si es necesario.\n",
        "\n",
        "    Args:\n",
        "        text (str): La cadena de texto a convertir.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Un tensor de PyTorch (`torch.LongTensor`) con los índices de las palabras.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    input_seq = torch.LongTensor([word_to_int[word] for word in words[-SEQUENCE_LENGTH:]]).unsqueeze(0)\n",
        "    return input_seq\n",
        "\n",
        "def sample_next(predictions):\n",
        "    \"\"\"\n",
        "    Realiza un muestreo \"greedy\" modificado, eligiendo aleatoriamente entre las dos\n",
        "    palabras más probables predichas por el modelo. Esto introduce un elemento de\n",
        "    aleatoriedad para generar haikus más \"creativos\" y variados.\n",
        "\n",
        "    Args:\n",
        "        predictions (torch.Tensor): El tensor de logits (`[batch_size, sequence_length, vocab_size]`)\n",
        "                                    de la salida del modelo.\n",
        "\n",
        "    Returns:\n",
        "        int: El índice de la palabra seleccionada.\n",
        "    \"\"\"\n",
        "    probabilities = F.softmax(predictions[:, -1, :], dim=-1).cpu()\n",
        "    values, indices = torch.topk(probabilities, k=2)\n",
        "    seed = random.randint(0,1)\n",
        "    if (seed == 0):\n",
        "        next_token = indices[0][0]\n",
        "    else:\n",
        "        next_token = indices[0][1]\n",
        "    return int(next_token.cpu())\n",
        "\n",
        "def haiku_generator(sentence, generate_length):\n",
        "    \"\"\"\n",
        "    Genera un haiku completo utilizando el modelo entrenado y la estrategia de muestreo `sample_next`.\n",
        "    Se enfoca en la \"creatividad\" al permitir cierta aleatoriedad en la selección de palabras\n",
        "    y asegura que el haiku generado tenga al menos tres versos.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): La palabra o frase inicial (prompt) para comenzar la generación.\n",
        "        generate_length (int): La longitud máxima de palabras a generar.\n",
        "\n",
        "    Returns:\n",
        "        str: El haiku generado como una cadena de texto, con los tokens de fin de secuencia (`$`)\n",
        "             y los separadores de verso (`/`) formateados para una mejor legibilidad.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    sample = sentence\n",
        "    for i in range(generate_length):\n",
        "        int_vector = return_int_vector(sample)\n",
        "        if len(int_vector) >= SEQUENCE_LENGTH - 1:\n",
        "            break\n",
        "        input_tensor = int_vector.to(device)\n",
        "        with torch.no_grad():\n",
        "            predictions = model(input_tensor)\n",
        "        next_token = sample_next(predictions)\n",
        "        sample += ' ' + int_to_word[next_token]\n",
        "        if (int_to_word[next_token] == '$'):\n",
        "            break\n",
        "    if (sample.count('/')!=2):\n",
        "      return haiku_generator(sentence, generate_length)\n",
        "    return sample.replace('$','').replace('/ ','\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una pequeña prueba para comprobar que si funciona."
      ],
      "metadata": {
        "id": "mI8DYhE0U737"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg9S0k197oR8",
        "outputId": "51d3ed18-ebfe-4e3f-fff6-3f92286b7150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT: i all\n",
            "i all the pain \n",
            "let me be free of you \n",
            "wind will be said \n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"i all\"\n",
        "]\n",
        "for sentence in sentences[0]:\n",
        "  if(sentence not in word_to_int):\n",
        "    if (sentence == ' '):\n",
        "      continue\n",
        "    print(f\"The word '{sentence}' is not in the vocabulary.\")\n",
        "generate_length = 100\n",
        "for sentence in sentences:\n",
        "    print(f\"PROMPT: {sentence}\")\n",
        "    print(haiku_generator(sentence, generate_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La interfaz gráfica hecha para probar el programa, el cuál es un tipo chatbot al que le escribes una palabra que pertenece al vocabulario como prompt y te genera un haiku basado en esa palabra."
      ],
      "metadata": {
        "id": "J418N9nBU_Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def haiku_creator(sentence):\n",
        "  \"\"\"\n",
        "  Función auxiliar que envuelve a `haiku_generator` para ser utilizada dentro de la interfaz de Gradio.\n",
        "  Simplifica la llamada a la función de generación de haikus con una longitud fija.\n",
        "\n",
        "  Args:\n",
        "      sentence (str): La palabra o frase de inicio (prompt).\n",
        "\n",
        "  Returns:\n",
        "      str: El haiku generado por `haiku_generator`.\n",
        "  \"\"\"\n",
        "  generate_length = 100\n",
        "  haiku_generator(sentence, generate_length)\n",
        "  return haiku_generator(sentence, generate_length)\n",
        "\n",
        "def chat_logic(message, history):\n",
        "    \"\"\"\n",
        "    Lógica principal del chatbot de Gradio. Recibe el mensaje del usuario (prompt para el haiku)\n",
        "    y el historial del chat. Valida que todas las palabras del prompt existan en el vocabulario\n",
        "    del modelo y luego llama a `haiku_creator` para generar un haiku.\n",
        "\n",
        "    Args:\n",
        "        message (str): La entrada actual del usuario (el prompt).\n",
        "        history (list): El historial de conversaciones del chatbot (no utilizado directamente aquí).\n",
        "\n",
        "    Returns:\n",
        "        str: El haiku generado como una cadena de texto.\n",
        "\n",
        "    Raises:\n",
        "        gr.Error: Si alguna palabra en el `message` del usuario no se encuentra en el vocabulario del modelo.\n",
        "    \"\"\"\n",
        "    word = message.strip().lower()\n",
        "    sentence = word.split()\n",
        "    #print(word)\n",
        "    for i in sentence:\n",
        "      if (i == ' '):\n",
        "        continue\n",
        "      if (i not in word_to_int):\n",
        "        raise gr.Error(f\" The word '{i}' is not in the vocabulary.\")\n",
        "\n",
        "    haiku = haiku_creator(word)\n",
        "    return haiku\n",
        "\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {min-height: 400vh;}\n",
        "footer {display: none !important;}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), css=custom_css, title=\"HaikuAI\") as demo:\n",
        "\n",
        "    chatbot = gr.Chatbot(\n",
        "        height=400,\n",
        "        placeholder=\"HAIKUAI\",\n",
        "        type=\"messages\",\n",
        "        bubble_full_width=True\n",
        "    )\n",
        "\n",
        "    chat_interface = gr.ChatInterface(\n",
        "        fn=chat_logic,\n",
        "        chatbot=chatbot,\n",
        "        textbox=gr.Textbox(placeholder=\"Type a word to create a Haiku\", container=True, scale=7),\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "TFcOaMVhzyrq",
        "outputId": "4f24ee22-5858-4b1d-9e14-beb71dbd817b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3728659577.py:51: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), css=custom_css, title=\"HaikuAI\") as demo:\n",
            "/tmp/ipython-input-3728659577.py:51: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), css=custom_css, title=\"HaikuAI\") as demo:\n",
            "/tmp/ipython-input-3728659577.py:53: DeprecationWarning: The 'bubble_full_width' parameter will be removed in Gradio 6.0. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-3728659577.py:53: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:330: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'messages', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://72540fea4ac6bbc79e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://72540fea4ac6bbc79e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias"
      ],
      "metadata": {
        "id": "vA-o5siTKugF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el desarrollo del proyecto se utilizaron diferentes ejemplos de la arquitectura del transformer. Además de cómo entrenar el modelo. Los links a los materiales usados son:\n",
        "\n",
        "* https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/blob/main/Notebooks/08Noam.ipynb\n",
        "\n",
        "* https://github.com/VictorMijangosDeLaCruz/MecanismosAtencion/tree/main/Notebooks\n",
        "\n",
        "* https://medium.com/@aadit.kshirsagar/building-a-text-generation-transformer-from-scratch-a-deep-dive-3dcde380013b\n",
        "\n",
        "* https://www.geeksforgeeks.org/nlp/understanding-bleu-and-rouge-score-for-nlp-evaluation/\n",
        "\n",
        "* https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html\n",
        "\n",
        "* https://debuggercafe.com/text-generation-with-transformers/\n",
        "\n",
        "* https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "* https://www.kaggle.com/datasets/bfbarry/haiku-dataset"
      ],
      "metadata": {
        "id": "L7jmAa3XKv_B"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "528cc55d5b1741919daa095ebaee4696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c460975c1dc2432a8843fec90e06a5e1",
              "IPY_MODEL_dbe6bf9e6db94744b5335e7612e399fb",
              "IPY_MODEL_54352b05d78d4276b685ea5aaba82453"
            ],
            "layout": "IPY_MODEL_5469188f5fdb46f4bfefd7f45064504c"
          }
        },
        "c460975c1dc2432a8843fec90e06a5e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_527088779b87474784f50370cd1a6c91",
            "placeholder": "​",
            "style": "IPY_MODEL_bed5dd1809854e9cad95e50ba42d492d",
            "value": "Downloading builder script: "
          }
        },
        "dbe6bf9e6db94744b5335e7612e399fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_021ae598b77f45049b9a40dfa6565d0b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5509ad41830047daa019611d39b9401a",
            "value": 1
          }
        },
        "54352b05d78d4276b685ea5aaba82453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e769f83f1bc84c559ca4399fc5887d69",
            "placeholder": "​",
            "style": "IPY_MODEL_05882c0b2bbb4cfd969006cafac52755",
            "value": " 6.14k/? [00:00&lt;00:00, 420kB/s]"
          }
        },
        "5469188f5fdb46f4bfefd7f45064504c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "527088779b87474784f50370cd1a6c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bed5dd1809854e9cad95e50ba42d492d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "021ae598b77f45049b9a40dfa6565d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5509ad41830047daa019611d39b9401a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e769f83f1bc84c559ca4399fc5887d69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05882c0b2bbb4cfd969006cafac52755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}